{"cells":[{"cell_type":"markdown","metadata":{},"source":["## VGGNet"]},{"cell_type":"markdown","metadata":{},"source":["### About VGGNet...\n","- Published as a conference paper at ICLR 2015 from the Visual Geometry Group (VGG) @ Oxford\n","- The original paper **Very Deep Convolutional Networks for Large-Scale Image Recognition** can be found at <https://arxiv.org/abs/1409.1556>\n","- Secured the first and second place in the localization and classification tracks (respectively) of ILSVRC 2014\n","- Proposed 4 variants of CNN architecture with 11, 13, 16 and 19 layers (Yes, 19 layers was as deep as they could go back in 2014!)"]},{"cell_type":"markdown","metadata":{},"source":["### Importing the necessary libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6574,"status":"ok","timestamp":1638132891958,"user":{"displayName":"Gokulakrishnan CANDASSAMY","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGyK-9yVRKWLm8l4bheSAj4FeQac-OwG0zRDd=s64","userId":"09874617787511296953"},"user_tz":480},"id":"iYh5oO0dNw46"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F"]},{"cell_type":"markdown","metadata":{},"source":["### Complete architecture...\n","\n","![](https://cdn-5f733ed3c1ac190fbc56ef88.closte.com/wp-content/uploads/2017/03/VGGNet.png \"VGG Architecture\")"]},{"cell_type":"markdown","metadata":{},"source":["### Defining architecture skeletons...\n","\n","- All the convolution layers use 3x3 filters with padding=1 and stride=1, thereby maintaining the spatial resolution\n","- The integers represent the number of filters to be used in a convolutional layer\n","- The letter 'M' represents the maxpooling layer with kernel_size=2 and stride=2 (i.e. it reduces the spatial resolution by a factor of 2)\n","- Designed to work with RGB images of size 224x224 (very common in ImageNet) aimed at 1000-class classification\n","- Though not included in the original paper, we will use BatchNormalization layer after every convolution layer and use Relu activations along with Dropout layers in the FC portion of the network\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":245,"status":"ok","timestamp":1638134894048,"user":{"displayName":"Gokulakrishnan CANDASSAMY","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGyK-9yVRKWLm8l4bheSAj4FeQac-OwG0zRDd=s64","userId":"09874617787511296953"},"user_tz":480},"id":"GzB_boMvVBND"},"outputs":[],"source":["VGG_archs = {\n","    'VGG11': [64, 'M',  128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n","}"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2349,"status":"ok","timestamp":1638134899139,"user":{"displayName":"Gokulakrishnan CANDASSAMY","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGyK-9yVRKWLm8l4bheSAj4FeQac-OwG0zRDd=s64","userId":"09874617787511296953"},"user_tz":480},"id":"d4NpDwI_VnKf","outputId":"8d9c3d78-56b8-4a89-c801-d5c98d20b504"},"outputs":[],"source":["class VGGNet(nn.Module):\n","  def __init__(self, in_channels=3, num_classes=1000):\n","    super(VGGNet, self).__init__()\n","    self.in_channels = in_channels\n","    self.conv_layers = self.create_conv_layers(VGG_archs['VGG16'])\n","    self.fcs = nn.Sequential(\n","        nn.Linear(512*7*7, 4096),\n","        nn.ReLU(),\n","        nn.Dropout(p=0.5),\n","\n","        nn.Linear(4096, 4096),\n","        nn.ReLU(),\n","        nn.Dropout(p=0.5),\n","\n","        nn.Linear(4096, num_classes)\n","        )\n","  \n","  def forward(self, x):\n","    x = self.conv_layers(x)\n","    x = x.reshape(x.shape[0], -1)\n","    x = self.fcs(x)\n","    return x\n","  \n","  def create_conv_layers(self, architecture):\n","    layers = []\n","    in_channels = self.in_channels\n","\n","    for layer in architecture:\n","      if type(layer) == int:\n","        out_channels = layer\n","\n","        layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n","                             kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n","                   nn.BatchNorm2d(out_channels),\n","                   nn.ReLU()]\n","        in_channels = out_channels #updating input channels for the next layer...\n","      elif layer == 'M':\n","        layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))]\n","\n","    print(len(layers)) #returns the total number of layers before the FC layers (including BN, activations, etc...)\n","    print(type(layers[0]), type(layers[1]), type(layers[2]), type(layers[6])) #should be Conv2d, BatchNorm2d, ReLU and MaxPool2d\n","    return nn.Sequential(*layers)"]},{"cell_type":"markdown","metadata":{},"source":["### Sanity check..."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"9dWWh7nMbvv5"},"outputs":[{"name":"stdout","output_type":"stream","text":["44\n","<class 'torch.nn.modules.conv.Conv2d'> <class 'torch.nn.modules.batchnorm.BatchNorm2d'> <class 'torch.nn.modules.activation.ReLU'> <class 'torch.nn.modules.pooling.MaxPool2d'>\n","torch.Size([5, 1000])\n"]}],"source":["model = VGGNet(in_channels=3, num_classes=1000)\n","x = torch.randn(5, 3, 224, 224)\n","print(model(x).shape)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNDXr1/9xaXXefhZVo4C8Wm","collapsed_sections":[],"name":"vggnet_tutorial.ipynb","provenance":[]},"kernelspec":{"display_name":"dl_env","language":"python","name":"dl_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
