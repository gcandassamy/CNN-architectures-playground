{"cells":[{"cell_type":"markdown","metadata":{},"source":["## ResNet"]},{"cell_type":"markdown","metadata":{},"source":["### About ResNet\n","- Published as a conference paper at CVPR 2016 from Microsoft Research (MSR)\n","- The original paper \"Deep Residual Learning for Image Recognition\" can be found at <https://arxiv.org/abs/1512.03385>\n","- One of the most cited papers in the modern deep learning era\n","- Secured first place in the classification track of ILSVRC 2015 and also sweeped the localization challenges in both ILSVRC 2015 / COCO 2015 competitions"]},{"cell_type":"markdown","metadata":{},"source":["### Motivation / Key ideas:\n","- Back in 2014 / 15, researchers were trying to push the limits to see how deep they could go with neural networks\n","- Motivated by the success of VGGNet / InceptionNet, they tried extending these architecture to become more deeper, but they saw that the network struggled to learn (or converge)\n","- In principle, they expected that **deeper networks should perform at least as good as their shallower counterparts**\n","- In other words, the deeper layers should learn the identity mapping if that is the best thing to do!\n","- Thus, they added the skip connections, so that the weights in the layers only have to learn the residuals $\\mathcal{F}(x)$\n","![](https://miro.medium.com/v2/resize:fit:1400/1*jNyv5wv-LyXfRb3Ye1q0jA.png \"Residual Learning\")\n","- These skip connections provide some kind of **gradient superhighways** during backprop for the gradients to flow without much attenuation, thereby alleviating the vanishing gradients problem!"]},{"cell_type":"markdown","metadata":{},"source":["### Importing the necessary libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6028,"status":"ok","timestamp":1638190280987,"user":{"displayName":"Gokulakrishnan CANDASSAMY","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGyK-9yVRKWLm8l4bheSAj4FeQac-OwG0zRDd=s64","userId":"09874617787511296953"},"user_tz":480},"id":"EVf5VsmEj9UR"},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"markdown","metadata":{},"source":["### Implementing the ResNet Architecture\n","- The authors propose five different variants of the ResNet architectures with 18, 34, 50, 101 and 152 layers\n","- The complete specification of these architectures is as follows:\n","![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20210403172705.png \"ResNet architectures\")\n","- We will be focusing on the deeper architectures, starting with 50 layer onwards (note from the above figure that the only difference between 50, 101 and 152 layer version is the number of times certain blocks get repeated!)\n","\n","\n","\n"," "]},{"cell_type":"markdown","metadata":{},"source":["### Implementation guidelines:\n","- We will first implement the **block** class which creates a block containing three conv layers: 1x1, 3x3 and 1x1\n","    - One thing to be noted here is that, in each of these blocks, the number of output channels is **4** times the number of input channels\n","- Then, we will implement the **ResNet** class which defines the full architecture following the above table "]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":140,"status":"ok","timestamp":1638194799279,"user":{"displayName":"Gokulakrishnan CANDASSAMY","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGyK-9yVRKWLm8l4bheSAj4FeQac-OwG0zRDd=s64","userId":"09874617787511296953"},"user_tz":480},"id":"gsjxyworkBwJ"},"outputs":[],"source":["class block(nn.Module): \n","  def __init__(self, in_channels, out_channels, id_downsample=None, stride=1): \n","    ''' \n","    Creates a single 3-layer block of resnet architecture (1x1 N filters --> 3x3 N filters --> 1x1 4N filters)\n","    Args:\n","    in_channels : number of input channels to the first 1x1 conv layer\n","    out_channels : number of output channels to the first 1x1 conv layer\n","    id_downsample : a conv layer that will help us match shapes (if they differ) to perform elementwise addition...\n","    '''\n","    super(block, self).__init__()\n","    self.expansion = 4\n","\n","    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n","    self.bn1 = nn.BatchNorm2d(out_channels)\n","    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n","    self.bn2 = nn.BatchNorm2d(out_channels)\n","    self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n","    self.bn3 = nn.BatchNorm2d(out_channels*self.expansion)\n","    self.relu = nn.ReLU()\n","    self.id_downsample = id_downsample\n","    \n","    if id_downsample:\n","      print(f'Creating block with in_ch: {in_channels}, out_ch: {out_channels} and downsampling: {type(id_downsample[0])}')\n","    else:\n","      print(f'Creating block with in_ch: {in_channels}, out_ch: {out_channels} and downsampling: {type(id_downsample)}')\n","  def forward(self, x):\n","    identity = x\n","\n","    x = self.relu(self.bn1(self.conv1(x)))\n","    x = self.relu(self.bn2(self.conv2(x)))\n","    x = self.bn3(self.conv3(x))\n","    #Now add the skip connection...\n","\n","    if self.id_downsample is not None:\n","      identity = self.id_downsample(identity)\n","\n","    x += identity\n","    x = self.relu(x)\n","    return x"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["class ResNet(nn.Module):\n","  def __init__(self, block, layers, image_channels, num_classes): \n","    '''\n","    Args:\n","    layers : a list that contains the number of times the each resnet layer need to be repeated (say [3, 4, 6, 3] for 50 layers)\n","    image_channels : 1 for Grayscale images and 3 for RGB images\n","    num_classes : number of output classes\n","    '''\n","    super(ResNet, self).__init__()\n","    self.in_channels = 64\n","    #Initial layers...\n","    self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n","    self.bn1 = nn.BatchNorm2d(64)\n","    self.relu = nn.ReLU()\n","    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","    #ResNet layers begin...\n","    self.layer1 = self._make_layer(block, layers[0], out_channels=64, stride=1) # at the end we will have 64 * 4 = 256 channels\n","    self.layer2 = self._make_layer(block, layers[1], out_channels=128, stride=2) # 128 * 4 = 512 channels\n","    self.layer3 = self._make_layer(block, layers[2], out_channels=256, stride=2) # 1024 channels\n","    self.layer4 = self._make_layer(block, layers[3], out_channels=512, stride=2) # 2048 channels\n","\n","    self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n","    self.fc = nn.Linear(512*4, num_classes)\n","\n","\n","  def forward(self,x):\n","    x = self.relu(self.bn1(self.conv1(x)))\n","    x = self.maxpool(x)\n","\n","    x = self.layer1(x)\n","    x = self.layer2(x)\n","    x = self.layer3(x)\n","    x = self.layer4(x)\n","\n","    x = self.avgpool(x)\n","    x = x.reshape(x.shape[0], -1)\n","    x = self.fc(x)\n","\n","    return x\n","\n","\n","  def _make_layer(self, block, num_residual_blocks, out_channels, stride):\n","    id_downsample = None\n","    layers = []\n","\n","    # We will do the necessary transformation using id_downsample if (i) we change the spatial dimensions or \n","    # (ii) we change the no of channels\n","    if stride!=1 or self.in_channels != out_channels*4:\n","      id_downsample = nn.Sequential(\n","          nn.Conv2d(self.in_channels, out_channels*4, kernel_size=1, stride = stride ),\n","          nn.BatchNorm2d(out_channels*4)\n","      )\n","    \n","    #Creating the first residual block that changes the no of channels...\n","    layers.append(block(self.in_channels, out_channels, id_downsample, stride))\n","    self.in_channels = out_channels * 4\n","\n","    #Creating the remaining blocks...\n","    for i in range(num_residual_blocks-1):\n","      layers.append(block(self.in_channels, out_channels))\n","\n","    return nn.Sequential(*layers)\n"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":150,"status":"ok","timestamp":1638194803905,"user":{"displayName":"Gokulakrishnan CANDASSAMY","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGyK-9yVRKWLm8l4bheSAj4FeQac-OwG0zRDd=s64","userId":"09874617787511296953"},"user_tz":480},"id":"JRThGVlr_Hyc"},"outputs":[],"source":["def ResNet50(img_channels=3, num_classes=1000):\n","  return ResNet(block, [3, 4, 6, 3], img_channels, num_classes)\n","\n","def ResNet101(img_channels=3, num_classes=1000):\n","  return ResNet(block, [3, 4, 23, 3], img_channels, num_classes)\n","\n","def ResNet152(img_channels=3, num_classes=1000):\n","  return ResNet(block, [3, 8, 36, 3], img_channels, num_classes)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Sanity check..."]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":137,"status":"ok","timestamp":1638194840477,"user":{"displayName":"Gokulakrishnan CANDASSAMY","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGyK-9yVRKWLm8l4bheSAj4FeQac-OwG0zRDd=s64","userId":"09874617787511296953"},"user_tz":480},"id":"2otIYflSAovG"},"outputs":[],"source":["def sanity_check():\n","  net = ResNet50()\n","  x = torch.rand(2, 3, 224, 224)\n","  y = net(x)\n","  print(y.shape)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1066,"status":"ok","timestamp":1638194842593,"user":{"displayName":"Gokulakrishnan CANDASSAMY","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGyK-9yVRKWLm8l4bheSAj4FeQac-OwG0zRDd=s64","userId":"09874617787511296953"},"user_tz":480},"id":"1dH8dZg-BSE9","outputId":"38c6451d-bdd2-4187-b536-d58a9997932a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating block with in_ch: 64, out_ch: 64 and downsampling: <class 'torch.nn.modules.conv.Conv2d'>\n","Creating block with in_ch: 256, out_ch: 64 and downsampling: <class 'NoneType'>\n","Creating block with in_ch: 256, out_ch: 64 and downsampling: <class 'NoneType'>\n","Creating block with in_ch: 256, out_ch: 128 and downsampling: <class 'torch.nn.modules.conv.Conv2d'>\n","Creating block with in_ch: 512, out_ch: 128 and downsampling: <class 'NoneType'>\n","Creating block with in_ch: 512, out_ch: 128 and downsampling: <class 'NoneType'>\n","Creating block with in_ch: 512, out_ch: 128 and downsampling: <class 'NoneType'>\n","Creating block with in_ch: 512, out_ch: 256 and downsampling: <class 'torch.nn.modules.conv.Conv2d'>\n","Creating block with in_ch: 1024, out_ch: 256 and downsampling: <class 'NoneType'>\n","Creating block with in_ch: 1024, out_ch: 256 and downsampling: <class 'NoneType'>\n","Creating block with in_ch: 1024, out_ch: 256 and downsampling: <class 'NoneType'>\n","Creating block with in_ch: 1024, out_ch: 256 and downsampling: <class 'NoneType'>\n","Creating block with in_ch: 1024, out_ch: 256 and downsampling: <class 'NoneType'>\n","Creating block with in_ch: 1024, out_ch: 512 and downsampling: <class 'torch.nn.modules.conv.Conv2d'>\n","Creating block with in_ch: 2048, out_ch: 512 and downsampling: <class 'NoneType'>\n","Creating block with in_ch: 2048, out_ch: 512 and downsampling: <class 'NoneType'>\n","torch.Size([2, 1000])\n"]}],"source":["sanity_check()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOkl+GU6wFPp9Vesv0+uRjr","collapsed_sections":[],"name":"ResNet.ipynb","provenance":[]},"kernelspec":{"display_name":"dl_env","language":"python","name":"dl_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
